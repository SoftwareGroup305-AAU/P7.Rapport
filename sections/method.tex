\section{Method}\label{sec:method}

\subsection{Framework} %Framework -> diagrammer "this is why is good" -> Dyk ned i componenter 
To support the investigation of structured hybrid time series prediction strategies, we develop an integrated framework in which the system's components operate together to enable sequential execution, evaluation, and comparison of forecasting methods\todo{methods / strategies}. The framework connects five principal components: 
\begin{enumerate}
    \item An LSTM model service responsible for machine-learning model training and prediction.
    \item A backtesting engine for sequential evaluation of strategies on historical data.
    \item A unified API layer that orchestrates data flow and provides standardized access to system functionality.
    \item A database for storage of time series oriented towards efficient datapoint querying.
    \item A frontend interface for strategy design, visual analysis and interaction.
\end{enumerate}
This design directly addresses the challenge identified in the problem statement: the difficulty of comparing, gauging, and generalizing hybrid forecasting strategies in the absence of a unifying structure. Our framework enables reproducible experiments, systematic composition of heterogeneous methods, and frequent, precise evaluation of forecasting accuracy across historical datasets. As such, it provides a practical and extensible foundation for researching modular hybrid time-series prediction strategies.


\subsection{Data Acquisition and Storage}\label{dataaquisition}\todo{commented out section}
% CHANGED FROM THIS %%
%Working with financial market data requires reliable data providers with high-quality data available. While many provider options are available, the primary considerations were Yahoo Finance, EODHD and Massive (formerly Polygon.io) for stock data, along with Coinbase and Binance for crypto currency data. For crypto currency data, the most consistent dataset, at least for Bitcoin (BTC), was sourced from Coinbase. Regarding stock data, the selected provider was Massive, as they provide access to consistent historical data across the American stock market. Due to both providers being free to use, along with their consistent data quality, made them the selected providers for the crypto currency market and stock market, respectively\todo{source for providers}.

%Since financial time series data is often represented through the Open-High-Low-Close (OHLC) format, and the aim of the project is to make a general purpose time series prediction framework, this format must be generalized. To represent financial market data as general purpose time series, each OHLC entry is split in multiple series, where each serie only has a single vector of data points representing each metric. Each serie represents the metadata concerning a set of data points, such as asset, metric, data set time span, and resolution of the time intervals. Each data point is represented as a value, a time interval and a corresponding serie identifier. This relation allows for a uniform way to represent time series, regardless of domain. All data points are single values at specific time frame, and the interpretation of the value depends on the metadata of the related serie.

%To store time series efficiently, the storage system would advantageously support high-volume data ingestion, as well as fast queries on large tables. This helps allow efficient addition of new time series with historical data, and update existing time series continuously. It also allows for responsive querying on data points during strategy backtesting. Furthermore, due to the relation between the series and data points, a relational database is preferable.

%For this task, ClickHouse was selected as the database management system. The relational column-oriented architecture of the storage system allows for fast indexing on tables with many entries, which in this case increases the efficiency of the data point extraction process \todo{source}. This efficient querying along with data types like high-precision floating point numbers and time formats, made ClickHouse a suitable choice as database management system for general purpose time series. 

%% TO THIS %%
\textbf{Data Providers.} Working with financial market data requires reliable providers that offer high-quality datasets. While many options exist, the primary candidates were Yahoo Finance, EODHD, and Massive (formerly Polygon.io) for stock data, along with Coinbase and Binance for cryptocurrency data.

For cryptocurrency, the most consistent dataset, specifically for Bitcoin, was sourced from Coinbase. Regarding stock data, Massive was selected as it provides access to consistent historical data across the American stock market. Because both providers are free to use and offer consistent data quality, they were selected for the cryptocurrency and stock markets, respectively \todo{source for providers}.

\textbf{Data Representation.} Since financial time series data is typically represented in the OHLCV format, and the aim of the project is to make a general-purpose time series prediction framework, this format must be generalized. To represent financial market data as general-purpose time series, each OHLCV entry is split into multiple series, where each series contains only a single vector of data points representing each metric.

Each series contains the metadata concerning a set of data points, such as the asset, metric, temporal coverage, and interval resolution. Each data point is represented as a value, a timestamp, and a corresponding series identifier. This relationship allows for a uniform way to represent time series regardless of domain. All data points are single values at a specific timestamp, and the interpretation of the value depends entirely on the metadata of the related series.

\textbf{Storage System.} To store time series efficiently, the system requires a solution capable of high-volume data ingestion and rapid querying of large tables. This helps allow efficient addition of new time series with historical data, as well as the continuous update of existing time series. It also allows for responsive querying of data points during strategy backtesting. Furthermore, due to the relationship between the series and data points, a relational database is preferable.

For this task, ClickHouse was selected as the database management system. The relational column-oriented architecture of the storage system allows for fast indexing on tables with many entries, which in this case increases the efficiency of the data point extraction process \todo{source}. This efficient querying, along with data types like high-precision floating-point numbers and time formats, made ClickHouse a suitable choice as a database management system for general-purpose time series.

\subsection{Strategy Composition Framework}

The design of the strategy components was based on widely adopted statistical models and data-manipulation techniques. These building blocks were integrated into a modular framework in which strategies are represented as DAGs. Each node performs a well-defined transformation or action, such as fetching data, computing indicators, evaluating conditions, or generating signals. Nodes are connected via typed inputs and outputs to produce end-to-end forecasting logic, which in the case of financial data results in taking positions.

\begin{itemize}
    \item \textbf{Assets:} Asset nodes retrieve historical price data from the Data Warehouse\todo{general api} API for a given asset, metric and time frame. In normal operation, an asset node provides only a single configured price metric from the OHLCV format. Complete OHLCV data is not sourced from the asset node itself but from a separate global setting responsible for supplying full market data required for accurate financial backtesting. Asset nodes therefore function as lightweight, metric-specific data feeds for all downstream components.
    \item \textbf{Indicators:} Indicator nodes compute statistical or technical indicators over the values provided by upstream asset or selector nodes. The framework includes common indicators such as SMA, EMA, RSI, Bollinger Bands, and MACD\todo{explain / reference explanation section}, each producing one or more time-aligned output series. These outputs may then be used directly or passed through selectors for offsetting or field extraction.
    \item \textbf{Selectors:} Selector nodes extract specific values from asset or indicator outputs, optionally applying a time offset (e.g., value from previous time step). They provide a unified mechanism for selecting fields such as \textit{Close}, \textit{upper} (Bollinger upper band)\todo{explain bollinger bands}, or any indicator output. Selectors prepare data for condition nodes by ensuring that each operand refers to the correct value and temporal index.
    \item \textbf{Conditions:}
Condition nodes evaluate boolean expressions over one or more selector outputs. Supported expressions include standard comparison operators ($>$, $<$, $\geq$, $\leq$, ==, !=) and logical operators ($\land$, $\lor$). The result is a boolean time series indicating when the specified condition holds, enabling constructs such as moving-average crossovers, breakout rules, or volatility filters\todo{potentially explain / reference explanation section}.
    \item \textbf{Signals:}
Signal nodes generate trading actions when their triggering condition is satisfied. Basic signal nodes produce directional entry instructions (long or short)\todo{explain entry}, while extended signal nodes additionally compute take-profit and stop-loss\todo{explain tp sl} levels derived from percentage offsets relative to the entry price. Each signal is tied to a target asset node to ensure correct execution context.
    \item \textbf{ML Prediction Nodes (ML-Nodes).}
ML-Nodes generate rolling machine-learning forecasts that update at every historical timestep. 
Each node receives a univariate price series from a preceding selector, which extracts a specific field (e.g., close price) from an asset's OHLCV data. Using this input, the ML-Node applies a pre-trained LSTM time-series model configured through parameters such as the asset, metric, time frame, model name, and prediction horizon.

The node operates in a walk-forward manner. A fixed start date defines the beginning of the training window, while the end of the window advances with each time step in the dataset\todo{is the start date fixed?}. At every time step $t$, the model loads its windowed training data and produces a vector of forecasts for the next $N$ periods (e.g., $\hat{p}_{t+1}, \hat{p}_{t+2}, \ldots, \hat{p}_{t+N}$). These predictions are stored as a nested structure, one forecast vector per historical time step, and can be specified using selector nodes.

ML-Nodes enable strategies to incorporate ML-based expectations of short-or medium-term price movements, which can be combined with conventional condition nodes or threshold rules to construct hybrid rule-based and AI-driven trading strategies.
\end{itemize}

A complete overview and breakdown in UML-form of the different components can be seen in appendix \todo{add to appendix}

\subsection{LSTM Model Specification}\label{method:lstm}

To capture non-linear temporal dependencies in financial time series, the system employs an LSTM network. LSTMs are a specialized variant of RNNs designed to address the vanishing gradient problem inherent in standard RNNs, which hampers the ability to learn long-term dependencies \cite{Hochreiter1997}.

\subsubsection{Theoretical Background}
Unlike standard feedforward networks, RNNs maintain an internal state (memory) that processes sequences of inputs. However, during backpropagation through time, gradients in standard RNNs tend to decay exponentially, making it difficult to propagate error signals over long sequences. \todo{SOURCE}

The LSTM architecture introduces a memory cell $C_t$ modulated by three distinct gating mechanisms: the forget gate $f_t$, the input gate $i_t$, and the output gate $o_t$. These gates regulate the flow of information, allowing the network to retain relevant historical information while eliminating noise.

The internal structure of the LSTM cell is illustrated in Figure \ref{fig:lstm_cell}. The corresponding transition equations for a single cell at time step $t$ are defined as follows:

\begin{align}
    f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
    i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
    \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
    C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
    o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
    h_t &= o_t \odot \tanh(C_t)
\end{align}

\begin{figure}[H]
\centering
    % \resizebox{width}{height}{content} - '!' maintains aspect ratio
    \resizebox{\linewidth}{!}{
        \begin{tikzpicture}[
            % Global styles
            >=Latex,
            node distance=1.5cm,
            layer/.style={rectangle, draw=black, thick, minimum width=0.8cm, minimum height=0.8cm, fill=white},
            op/.style={circle, draw=black, thick, minimum size=0.5cm, fill=white, inner sep=0pt},
            connect/.style={->, thick},
            box/.style={rectangle, draw=black!50, dashed, rounded corners, inner sep=0.5cm}
        ]

        % --- Inputs ---
        \node (xt) at (0, -3.5) {$x_t$};
        \node (ht_prev) at (-2, -3.5) {$h_{t-1}$};
        \node (Ct_prev) at (-4.5, 3) {$C_{t-1}$};

        % --- The Gates (Bottom Row) ---
        % Forget Gate
        \node[layer] (sigma_f) at (-3, 0) {$\sigma$};
        \node[above=0.1cm of sigma_f] {\footnotesize $f_t$};
        
        % Input Gate
        \node[layer] (sigma_i) at (-1, 0) {$\sigma$};
        \node[above=0.1cm of sigma_i] {\footnotesize $i_t$};
        
        % Candidate Cell
        \node[layer] (tanh_c) at (1, 0) {$\tanh$};
        \node[above=0.1cm of tanh_c] {\footnotesize $\tilde{C}_t$};
        
        % Output Gate
        \node[layer] (sigma_o) at (3, 0) {$\sigma$};
        \node[above=0.1cm of sigma_o] {\footnotesize $o_t$};

        % --- Internal Operations ---
        
        % Input processing multiplication (i_t * C~_t)
        \node[op] (mul_i) at (0, 1.5) {$\times$};

        % Cell State Operations (Top Rail)
        \node[op] (mul_f) at (-3, 3) {$\times$}; % Forget operation
        \node[op] (add_c) at (0, 3) {$+$};      % Update operation
        
        % Output processing (Shifted right to avoid overlap)
        \node[layer] (tanh_h) at (4.5, 1.5) {$\tanh$};
        \node[op] (mul_h) at (4.5, 0) {$\times$};

        % --- Outputs ---
        \node (ht) at (6, 0) {$h_t$};
        \node (Ct) at (6, 3) {$C_t$};

        % --- Connections ---

        % 1. Input Distribution Flow
        % Merge inputs
        \draw[thick] (xt) -- (0, -2.5);
        \draw[thick] (ht_prev) |- (0, -2.5);
        
        % Distribute to gates (Horizontal line at y=-2.5)
        \draw[thick] (-3, -2.5) -- (3, -2.5);
        \draw[connect] (0, -2.5) -- (0, -2.5); % Just to ensure continuity
        
        % Up to gates
        \draw[connect] (-3, -2.5) -- (sigma_f);
        \draw[connect] (-1, -2.5) -- (sigma_i);
        \draw[connect] (1, -2.5) -- (tanh_c);
        \draw[connect] (3, -2.5) -- (sigma_o);

        % 2. Forget Gate Flow
        \draw[connect] (Ct_prev) -- (mul_f);
        \draw[connect] (sigma_f) -- (mul_f);

        % 3. Input Gate Flow
        \draw[connect] (sigma_i) |- (mul_i);
        \draw[connect] (tanh_c) |- (mul_i);
        \draw[connect] (mul_i) -- (add_c);

        % 4. Cell State Update Flow
        \draw[connect] (mul_f) -- (add_c);
        \draw[connect] (add_c) -- (Ct);
        
        % 5. Output Gate Flow
        % Branch from Cell State to tanh_h
        \draw[connect] (4.5, 3) -- (tanh_h); 
        % Connect tanh_h to final mult
        \draw[connect] (tanh_h) -- (mul_h);
        % Connect Output Gate to final mult
        \draw[connect] (sigma_o) -- (mul_h);
        % Final output
        \draw[connect] (mul_h) -- (ht);

        % Bounding box for the cell
        \node[box, fit=(sigma_f) (sigma_o) (mul_f) (add_c) (tanh_h) (mul_h)] {};

        \end{tikzpicture}
    }
    \caption{Internal architecture of the LSTM memory cell. The diagram illustrates the flow of data through the forget gate ($f_t$), input gate ($i_t$), and output gate ($o_t$), corresponding to Equations (1)--(6). The output processing is separated to show how the updated cell state $C_t$ is passed through a $\tanh$ activation and filtered by the output gate to produce the hidden state $h_t$.}
    \label{fig:lstm_cell}
\end{figure}

In these equations and the corresponding diagram, $\sigma$ denotes the sigmoid activation function which outputs values between 0 and 1, effectively acting as a soft switch that regulates information flow. The $\tanh$ activation function compresses values between -1 and 1, used here for generating candidate memory updates and normalizing the output. The operator $\odot$ represents element-wise multiplication or Hadamard product, while $W$ and $b$ represent the learnable weight matrices and bias vectors, respectively. The input vector $x_t$ contains the features at the current time step, and $h_{t-1}$ represents the short-term state from the previous step.

The diagram visualizes how these operations interact: the forget gate $f_t$ determines what percentage of the long-term memory $C_{t-1}$ to retain, while the input gate $i_t$ scales the new candidate information $\tilde{C}_t$ before adding it to the cell state. Finally, the output gate $o_t$ filters the updated cell state $C_t$, after passing through a $\tanh$ activation, to produce the new hidden state $h_t$. The uninterrupted flow of the cell state $C_t$ along the top rail of the diagram allows gradients to propagate during backpropagation, which mitigates the vanishing gradient problem. 

\subsubsection{Network Architecture}
The implemented model architecture is a stacked LSTM\todo{stacked vs unstacked explanation} network designed for regression. It consists of the following layers:
\begin{itemize}
    \item \textbf{Input Layer:} Accepts a sequence of shape $(k, 1)$, where $k$ is the lookback window size.
    \item \textbf{Hidden Layers:} Three stacked LSTM layers with 50 units each. The first two layers return full sequences to preserve temporal structure, while the final LSTM layer returns only the last hidden state vector.
    \item \textbf{Dropout:} A dropout rate (e.g., 0.2) is applied between layers to prevent overfitting.
    \item \textbf{Output Layer:} A single Dense (fully connected) layer with a linear activation function to produce the scalar forecast $\hat{y}_{t+1}$.
\end{itemize}
The model is optimized using the Adam algorithm, minimizing the Mean Squared Error (MSE) loss function.\todo{how does Adam minimize loss?}

\subsubsection{Preprocessing}
Financial time series are often non-stationary. To stabilize the mean and variance, raw OHLCV closing prices $P_t$ are transformed into logarithmic returns:
\begin{equation}
    r_t = \ln\left(\frac{P_t}{P_{t-1}}\right)
\end{equation}
Subsequently, the data is normalized to the range $[-1, 1]$ using Min-Max scaling to ensure stable convergence during gradient descent.

\subsubsection{Training and Inference}
The training process utilizes a sliding window technique. For a chosen lookback period $k$, the model learns to map a sequence of historical returns $X_t = \{r_{t-k}, \dots, r_{t-1}\}$ to the next target return $r_t$. The complete training procedure is outlined in Algorithm \ref{alg:lstm_training}.

\begin{algorithm}[h]
\caption{LSTM Training Procedure}
\label{alg:lstm_training}
\begin{algorithmic}[1]
\REQUIRE Historical Prices $P = \{p_1, \dots, p_T\}$, Lookback $k$, Epochs $E$
\ENSURE Trained LSTM Model $\mathcal{M}$, Scaler $\mathcal{S}$

\STATE \textbf{1. Preprocessing}
\STATE $R \leftarrow \text{ComputeLogReturns}(P)$ \COMMENT{$r_t = \ln(p_t / p_{t-1})$}
\STATE $\mathcal{S} \leftarrow \text{FitMinMaxScaler}(R, \text{range}=[-1, 1])$
\STATE $R' \leftarrow \text{Transform}(R, \mathcal{S})$
\STATE $\mathcal{D}_{train}, \mathcal{D}_{val} \leftarrow \text{SplitData}(R')$

\STATE \textbf{2. Sequence Generation}
\FOR{$t = k$ \TO length($\mathcal{D}_{train}$)}
    \STATE $x_t \leftarrow \{r'_{t-k}, \dots, r'_{t-1}\}$
    \STATE $y_t \leftarrow r'_t$
    \STATE Add $(x_t, y_t)$ to $\mathcal{X}_{train}, \mathcal{Y}_{train}$
\ENDFOR

\STATE \textbf{3. Model Initialization}
\STATE $\mathcal{M} \leftarrow \text{InitializeLSTM}(\text{layers}=3, \text{units}=50)$
\STATE $\text{Compile}(\mathcal{M}, \text{optimizer}=\text{'Adam'}, \text{loss}=\text{'MSE'})$

\STATE \textbf{4. Training Loop}
\FOR{$epoch = 1$ \TO $E$}
    \FORALL{batch $B$ in $(\mathcal{X}_{train}, \mathcal{Y}_{train})$}
        \STATE $\hat{Y} \leftarrow \text{ForwardPass}(\mathcal{M}, B_x)$
        \STATE $L \leftarrow \text{MSE}(B_y, \hat{Y})$
        \STATE $\text{Backpropagate}(L)$
        \STATE $\text{UpdateWeights}(\mathcal{M})$
    \ENDFOR
    \STATE $L_{val} \leftarrow \text{Evaluate}(\mathcal{M}, \mathcal{D}_{val})$
    \IF{$L_{val}$ plateaus}
        \STATE $\text{ReduceLearningRate}()$
    \ENDIF
\ENDFOR
\RETURN $\mathcal{M}, \mathcal{S}$
\end{algorithmic}
\end{algorithm}



To ensure continuous model improvement during training, the dataset is split into training data and validation data. At the end of each epoch, the model is exposed to the validation data, and the loss is evaluated, but not backpropagated. This loss should ideally gradually decrease. If it increases continuously while the training loss decreases, it indicates training data memorization and model overfitting. To ensure smooth learning throughout the epochs, a learning rate scheduler is employed, resulting in an initial sharp increase and later gradual decrease of the learning rate, allowing heavy adjustments early on in training, and fine-tuning later on. Inference is performed in a walk-forward manner, ensuring that predictions at time $t$ rely solely on information available up to $t-1$.

For the JPMorgan Chase (JPM)\todo{why jpm} experiments, the LSTM model was trained on daily price data spanning 2020--2023, providing four years of diverse market conditions including the COVID-19 crash, subsequent recovery, and early Federal Reserve tightening cycles. This extended training window enables the model to learn generalizable patterns across multiple market regimes. The trained model was then evaluated exclusively on out-of-sample data from 2024 to December 2025, ensuring strict temporal separation between training and testing periods to prevent data leakage and provide an unbiased assessment of predictive performance.

\subsection{Experimental Design}
To validate the efficacy of hybrid strategies and investigate whether statistical indicators can function as `regime filters` for machine-learning predictions, we constructed three distinct classes of trading strategies using the node-based architecture. These archetypes represent fundamentally different approaches to algorithmic trading and enable controlled comparison between traditional, ML-driven, and hybrid methodologies.

\subsubsection{Strategy Archetypes}
\begin{enumerate}
    \item \textbf{Statistical Control:} These strategies rely exclusively on \textit{Asset}, \textit{Indicator}, \textit{Selector}, and \textit{Condition} nodes, representing traditional rule-based algorithmic trading. Condition nodes evaluate relationships between technical indicators to generate trading signals without any machine learning component. Examples include Moving Average crossovers (e.g., SMA 20 crosses above SMA 50), Mean Reversion strategies based on Bollinger Bands, and momentum filters using the Relative Strength Index (RSI)\todo{financial terms, reference explanation section}. These strategies establish a baseline for what can be achieved with purely statistical logic.
    
    \item \textbf{ML-Only Baseline:} This class isolates the predictive capability of the \textit{ML-Nodes} by creating a direct pipeline from prediction to execution. The strategy graph consists of an Asset node feeding a Selector (to extract close price\todo{why specifically close}), connected to an ML-Node producing LSTM forecasts, which then connects to a Condition node that evaluates whether the predicted price $\hat{p}_{t+n}$ exceeds the current price $p_t$ by a configurable threshold percentage. When this condition is satisfied, a Signal node generates a directional trade. This archetype measures the `raw` performance of the deep learning model without any statistical constraints, representing an aggressive, prediction-driven approach.

    \item \textbf{Hybrid Experimental:} The primary experimental contribution involves Hybrid strategies that combine ML-Node predictions with statistical constraints within compound Condition nodes. Rather than treating statistical indicators as independent signal generators, these strategies use them as "regime filters" or "confirmation layers" for ML predictions. The Condition node evaluates a boolean expression that requires both the ML prediction criterion \textit{and} a statistical criterion to be satisfied simultaneously (e.g., \texttt{(predicted > current * 1.02) AND (price > SMA\_200)}). This topology\todo{n√∏rd} tests the hypothesis that statistical context, such as trend direction, momentum state, or volatility regime\todo{financial term}, can reduce false positives in volatility-sensitive machine learning models while preserving high-confidence signals.
\end{enumerate}

\subsubsection{Datasets}\label{datasets}
We selected two datasets to test performance under different volatility regimes:
\begin{itemize}
    \item \textbf{Bitcoin (BTC/USD, 2020--2025):} Represents a high-volatility, sentiment-driven asset class characterized by rapid price swings, asymmetric risk/reward profiles, and frequent regime changes between trending and mean-reverting behavior. The crypto market's 24/7 operation and susceptibility to external news shocks create a challenging environment where noise and signal are difficult to distinguish. This dataset enables evaluation of whether statistical filters can stabilize ML predictions during periods of extreme volatility and drawdown.
    \item \textbf{JPMorgan Chase (JPM, 2020--2025):} Represents a fundamentally-driven\todo{omformuler ? https://reverb.com/item/93460927-walrus-audio-fundamental-drive}, low-volatility equity. Daily price movements are typically constrained, and the asset exhibits lower beta relative to broad market indices. This stability introduces a different challenge: in low-noise environments, overly sensitive ML models may generate excessive false signals from minor fluctuations, while statistical filters risk over-constraining valid predictions. The JPM dataset serves as a validation case to determine whether the hybrid approach generalizes beyond crypto-specific dynamics to traditional equity markets. The model training period spans 2020--2023, with backtesting performed on strictly out-of-sample data from 2024 to December 2025.
\end{itemize}

By comparing results across these two different asset classes, the study aims to identify whether the value of statistical filtering is universal or regime-dependent, and whether the optimal filter type (trend, momentum, volatility)\todo{financial terms, reference to explanation section} varies with the underlying market structure.

\subsection{Process}\label{method:process}

To ensure efficient and collaborative group effort and contribution throughout the project development period, multiple management and leadership initiatives were introduced. Based on prior experience, a Kanban board was decided as the primary tool used to keep track of tasks and contributions to the project. Each item on the board represents a single task of varying granularity, all categorized based on which major component of the project it belongs to. Tasks are progressively assigned to available group members, which not only helps keep a general overview of distribution of knowledge and responsibilities, but also helps the individual group member keep track of personal tasks which they are currently working on or plan to work on.

The responsibility and facilitation of the management initiatives, along with encouragement of alignment and general adherence to the project goals, is primarily delegated to an implicit team leader. The aim is to implement the initiatives in the daily development cycle, and keep the project on track to addressing the problem statement.

Due to the modular architecture of the overall framework, which includes numerous components with different technical focuses, the usage of multiple repositories was implemented. This approach aims to reduce the number of merge conflicts experienced, which at times can greatly increase development speed. Furthermore, modules can separately be adjusted, updated and deployed through individual Continuous Deployment (CD) pipelines, such that overall functionality of the system is not interrupted by potential errors in single components. If a single component fails, multiple others are still available and functional, and are not directly affected by the changes of other components. This approach maximizes functionality and helps facilitate a continued scalable and modular development.