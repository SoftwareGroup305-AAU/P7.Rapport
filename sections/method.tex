\section{Method}


\todo{Afsnittet skal være mere detaljeorienteret og forklare anvendt teori}
\todo{Begrund opstilling / framework af projekt}
\todo{Gennemgå funktionaliteten af hvert komponent i projektet}
\subsection{Framework} %Framework -> diagrammer "this is why is good" -> Dyk ned i componenter 
To support the investigation of structured hybrid time series prediction strategies, we develop an integrated framework in which the system's componenets operate together to enable sequential execution, evaluation, and comparison of forecasting methods. The framework connects four principal components: 
\begin{enumerate}
    \item An LSTM model responsible for machine learning-based predictions.
    \item A backtesting engine for sequential evaluation of strategies on historical data
    \item A unified API layer that orchestrates data flow and provides standardized access to system functionality.
    \item A frontend interface for strategy design, visual analysis and interaction.
\end{enumerate}
This design directly addresses the challenge identified in the problem statement: the difficulty of comparing, gauging, and generalizing hybrid forecasting strategies in the absence of a unifying structure. Our framework enables reproducible experiments, systematic composition of heterogeneous methods, and frequent, precise evaluation of forecasting accuracy across historical datasets. As such, it provides a practical and extensible foundation for researching modular hybrid time-series prediction strategies.


\subsection{Data Acquisition and Storage}

Working with financial market data requires reliable data providers with high-quality data available. While many provider options are available, the primary considerations were Yahoo Finance, EODHD and Massive (formerly Polygon.io) for stock data, along with Coinbase and Binance for crypto currency data. For crypto currency data, the most consistent dataset, at least for Bitcoin (BTC), was sourced from Coinbase. Regarding stock data, the selected provider was Massive, as they provide access to consistent historical data across the American stock market. Due to both providers being free to use, along with their consistent data quality, made them the selected providers for the crypto currency market and stock market, respectively\todo{source for providers}.

Since financial time series data is often represented through the Open-High-Low-Close (OHLC) format, and the aim of the project is to make a general purpose time series prediction framework, this format must be generalized. To represent financial market data as general purpose time series, each OHLC entry is split in multiple series, where each serie only has a single vector of data points representing each metric. Each serie represents the metadata concerning a set of data points, such as asset, metric, data set time span, and resolution of the time intervals. Each data point is represented as a value, a time interval and a corresponding serie identifier. This relation allows for a uniform way to represent time series, regardless of domain. All data points are single values at specific time frame, and the interpretation of the value depends on the metadata of the related serie.

To store time series efficiently, the storage system would advantageously support high-volume data ingestion, as well as fast queries on large tables. This helps allow efficient addition of new time series with historical data, and update existing time series continuously. It also allows for responsive querying on data points during strategy backtesting. Furthermore, due to the relation between the series and data points, a relational database is preferable.

For this task, ClickHouse was selected as the database management system. The relational column-oriented architecture of the storage system allows for fast indexing on tables with many entries, which in this case increases the efficiency of the data point extraction process \todo{source}. This efficient querying along with data types like high-precision floating point numbers and time formats, made ClickHouse a suitable choice as database management system for general purpose time series. 

\subsection{Strategy Composition Framework}

The design of the strategy components was based on widely adopted statistical models and data-manipulation techniques. These building blocks were integrated into a modular framework in which strategies are represented as Directed Acyclic Graphs (DAGs). Each node performs a well-defined transformation, such as fetching data, computing indicators, evaluating conditions, or generating signals. Nodes are connected via typed inputs and outputs to produce end-to-end trading logic.

\begin{itemize}
    \item \textbf{Assets:} Asset nodes retrieve historical price data from the Data Warehouse API for a given trading pair and timeframe. In normal operation, an asset node provides only a single configured price metric (Open, High, Low, or Close). Complete OHLCV candlestick data is not sourced from the asset node itself but from a separate global setting responsible for supplying full market data required for accurate financial backtesting. Asset nodes therefore function as lightweight, metric-specific data feeds for all downstream components.
    \item \textbf{Indicators:} Indicator nodes compute statistical or technical indicators over the values provided by upstream asset or selector nodes. The framework includes common indicators such as SMA, EMA, RSI, Bollinger Bands, and MACD, each producing one or more time-aligned output series. These outputs may then be used directly or passed through selectors for offsetting or field extraction.
    \item \textbf{Selectors:} Selector nodes extract specific values from asset or indicator outputs, optionally applying a time offset (e.g., previous bar). They provide a unified mechanism for selecting fields such as \textit{Close}, \textit{upper} (Bollinger upper band), or any indicator output. Selectors prepare data for condition nodes by ensuring that each operand refers to the correct value and temporal index.
    \item \textbf{Conditions:}
Condition nodes evaluate boolean expressions over one or more selector outputs. Supported expressions include standard comparison operators ($>$, $<$, $\geq$, $\leq$, ==, !=) and logical operators ($\land$, $\lor$). The result is a boolean time series indicating when the specified condition holds, enabling constructs such as moving-average crossovers, breakout rules, or volatility filters.
    \item \textbf{Signals:}
Signal nodes generate trading actions when their triggering condition is satisfied. Basic signal nodes produce directional entry instructions (long or short), while extended signal nodes additionally compute take-profit and stop-loss levels derived from percentage offsets relative to the entry price. Each signal is tied to a target asset node to ensure correct execution context.
    \item \textbf{AI Prediction Nodes (AI-Nodes).}
AI-Nodes generate rolling machine-learning forecasts that update at every historical timestep. 
Each node receives a univariate price series from a preceding selector, which extracts a specific field (e.g., close price) from an asset's candlestick data. Using this input, the AINode applies a pre-trained LSTM time-series model configured through parameters such as the trading pair, timeframe, model name, and prediction horizon.

The node operates in a walk-forward manner. A fixed start date defines the beginning of the training window, while the end of the window advances with each bar in the dataset. At every bar $t$, the model loads its windowed training data and produces a vector of forecasts for the next $N$ periods (e.g., $\hat{p}_{t+1}, \hat{p}_{t+2}, \ldots, \hat{p}_{t+N}$). These predictions are stored as a nested structure, one forecast vector per historical bar, and can be specified using selector nodes.

AI-Nodes enable strategies to incorporate ML-based expectations of short-or medium-term price movements, which can be combined with conventional condition nodes or threshold rules to construct hybrid rule-based and AI-driven trading strategies.

A full overview and breakdown in UML-form of the different components can be seen in appendix \ref{} \todo{add to appendix}
\end{itemize}

\subsection{LSTM Model Specification}

To model non-linear dependencies, the system utilizes a Long Short-Term Memory (LSTM) network implemented in TensorFlow \cite{Hochreiter1997}. 
\todo{LSTM pseudokode}

\subsubsection{Preprocessing}
To address the non-stationary nature of financial data, raw prices are converted into logarithmic returns ($r_t = \ln(P_t / P_{t-1})$). These returns are then normalized via Min-Max scaling to the range $[0, 1]$ to facilitate gradient convergence.

\subsubsection{Training and Inference}
The model utilizes a sliding window approach. A lookback window of size $k$ is used to predict the next target value. The training process is structured as a walk-forward validation: the model is trained on a fixed historical window, and predictions are generated sequentially for the subsequent unseen data points. This ensures that no future information leaks into the prediction at time $t$.

\subsection{Experimental Design}
To validate the efficacy of hybrid strategies, we designed an experiment comparing three distinct strategy archetypes across two contrasting asset classes.

\subsubsection{Strategy Archetypes}
\begin{enumerate}
    \item \textbf{Statistical Control:} Strategies relying solely on deterministic logic (Asset $\to$ Indicator $\to$ Condition $\to$ Signal). Examples include SMA crossovers and RSI thresholds. This establishes a baseline for traditional algorithmic trading performance.
    \item \textbf{AI-Only Baseline:} A direct pipeline where the LSTM prediction triggers trades. A signal is generated if the predicted future price $\hat{p}_{t+n}$ exceeds the current price $p_t$ by a defined threshold. This measures the raw predictive power of the model without constraints.
    \item \textbf{Hybrid Experimental:} Strategies that gate AI predictions with statistical filters \cite{vanbekkum2021modular}. A trade is only executed if the AI predicts a move \textit{AND} a statistical condition confirms the market regime (e.g., Price $>$ SMA 200). This tests the hypothesis that technical analysis can reduce false positives in ML predictions.
\end{enumerate}

\subsubsection{Datasets}
We selected two datasets to test performance under different volatility regimes:
\begin{itemize}
    \item \textbf{Bitcoin (BTC/USD, 2020--2025):} Represents a high-volatility, noisy environment with frequent regime shifts.
    \item \textbf{Lockheed Martin (LMT, 2024--2025):} Represents a stable, low-volatility equity environment to test model generalization outside the cryptocurrency domain.
\end{itemize}

Approximately 340 unique configurations were backtested across these datasets to analyze the impact of different filter types and prediction horizons.
