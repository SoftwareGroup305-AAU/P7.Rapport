\section{Method}\label{sec:method}


\todo{Afsnittet skal være mere detaljeorienteret og forklare anvendt teori}
\todo{Begrund opstilling / framework af projekt}
\todo{Gennemgå funktionaliteten af hvert komponent i projektet}
\subsection{Framework} %Framework -> diagrammer "this is why is good" -> Dyk ned i componenter 
To support the investigation of structured hybrid time series prediction strategies, we develop an integrated framework in which the system's components operate together to enable sequential execution, evaluation, and comparison of forecasting methods. The framework connects four principal components: 
\begin{enumerate}
    \item An LSTM model responsible for machine learning-based predictions.
    \item A backtesting engine for sequential evaluation of strategies on historical data.
    \item A unified API layer that orchestrates data flow and provides standardized access to system functionality.
    \item A frontend interface for strategy design, visual analysis and interaction.
\end{enumerate}
This design directly addresses the challenge identified in the problem statement: the difficulty of comparing, gauging, and generalizing hybrid forecasting strategies in the absence of a unifying structure. Our framework enables reproducible experiments, systematic composition of heterogeneous methods, and frequent, precise evaluation of forecasting accuracy across historical datasets. As such, it provides a practical and extensible foundation for researching modular hybrid time-series prediction strategies.


\subsection{Data Acquisition and Storage}

Working with financial market data requires reliable data providers with high-quality data available. While many provider options are available, the primary considerations were Yahoo Finance, EODHD and Massive (formerly Polygon.io) for stock data, along with Coinbase and Binance for crypto currency data. For crypto currency data, the most consistent dataset, at least for Bitcoin (BTC), was sourced from Coinbase. Regarding stock data, the selected provider was Massive, as they provide access to consistent historical data across the American stock market. Due to both providers being free to use, along with their consistent data quality, made them the selected providers for the crypto currency market and stock market, respectively\todo{source for providers}.

Since financial time series data is often represented through the Open-High-Low-Close (OHLC) format, and the aim of the project is to make a general purpose time series prediction framework, this format must be generalized. To represent financial market data as general purpose time series, each OHLC entry is split in multiple series, where each serie only has a single vector of data points representing each metric. Each serie represents the metadata concerning a set of data points, such as asset, metric, data set time span, and resolution of the time intervals. Each data point is represented as a value, a time interval and a corresponding serie identifier. This relation allows for a uniform way to represent time series, regardless of domain. All data points are single values at specific time frame, and the interpretation of the value depends on the metadata of the related serie.

To store time series efficiently, the storage system would advantageously support high-volume data ingestion, as well as fast queries on large tables. This helps allow efficient addition of new time series with historical data, and update existing time series continuously. It also allows for responsive querying on data points during strategy backtesting. Furthermore, due to the relation between the series and data points, a relational database is preferable.

For this task, ClickHouse was selected as the database management system. The relational column-oriented architecture of the storage system allows for fast indexing on tables with many entries, which in this case increases the efficiency of the data point extraction process \todo{source}. This efficient querying along with data types like high-precision floating point numbers and time formats, made ClickHouse a suitable choice as database management system for general purpose time series. 

\subsection{Strategy Composition Framework}

The design of the strategy components was based on widely adopted statistical models and data-manipulation techniques. These building blocks were integrated into a modular framework in which strategies are represented as Directed Acyclic Graphs (DAGs). Each node performs a well-defined transformation, such as fetching data, computing indicators, evaluating conditions, or generating signals. Nodes are connected via typed inputs and outputs to produce end-to-end trading logic.

\begin{itemize}
    \item \textbf{Assets:} Asset nodes retrieve historical price data from the Data Warehouse API for a given trading pair and timeframe. In normal operation, an asset node provides only a single configured price metric (Open, High, Low, or Close). Complete OHLCV candlestick data is not sourced from the asset node itself but from a separate global setting responsible for supplying full market data required for accurate financial backtesting. Asset nodes therefore function as lightweight, metric-specific data feeds for all downstream components.
    \item \textbf{Indicators:} Indicator nodes compute statistical or technical indicators over the values provided by upstream asset or selector nodes. The framework includes common indicators such as SMA, EMA, RSI, Bollinger Bands, and MACD, each producing one or more time-aligned output series. These outputs may then be used directly or passed through selectors for offsetting or field extraction.
    \item \textbf{Selectors:} Selector nodes extract specific values from asset or indicator outputs, optionally applying a time offset (e.g., previous bar). They provide a unified mechanism for selecting fields such as \textit{Close}, \textit{upper} (Bollinger upper band), or any indicator output. Selectors prepare data for condition nodes by ensuring that each operand refers to the correct value and temporal index.
    \item \textbf{Conditions:}
Condition nodes evaluate boolean expressions over one or more selector outputs. Supported expressions include standard comparison operators ($>$, $<$, $\geq$, $\leq$, ==, !=) and logical operators ($\land$, $\lor$). The result is a boolean time series indicating when the specified condition holds, enabling constructs such as moving-average crossovers, breakout rules, or volatility filters.
    \item \textbf{Signals:}
Signal nodes generate trading actions when their triggering condition is satisfied. Basic signal nodes produce directional entry instructions (long or short), while extended signal nodes additionally compute take-profit and stop-loss levels derived from percentage offsets relative to the entry price. Each signal is tied to a target asset node to ensure correct execution context.
    \item \textbf{AI Prediction Nodes (AI-Nodes).}
AI-Nodes generate rolling machine-learning forecasts that update at every historical timestep. 
Each node receives a univariate price series from a preceding selector, which extracts a specific field (e.g., close price) from an asset's candlestick data. Using this input, the AINode applies a pre-trained LSTM time-series model configured through parameters such as the trading pair, timeframe, model name, and prediction horizon.

The node operates in a walk-forward manner. A fixed start date defines the beginning of the training window, while the end of the window advances with each bar in the dataset. At every bar $t$, the model loads its windowed training data and produces a vector of forecasts for the next $N$ periods (e.g., $\hat{p}_{t+1}, \hat{p}_{t+2}, \ldots, \hat{p}_{t+N}$). These predictions are stored as a nested structure, one forecast vector per historical bar, and can be specified using selector nodes.

AI-Nodes enable strategies to incorporate ML-based expectations of short-or medium-term price movements, which can be combined with conventional condition nodes or threshold rules to construct hybrid rule-based and AI-driven trading strategies.
\end{itemize}

A full overview and breakdown in UML-form of the different components can be seen in appendix \todo{add to appendix}

\subsection{LSTM Model Specification}

To capture non-linear temporal dependencies in financial time series, the system employs a Long Short-Term Memory (LSTM) network. LSTMs are a specialized variant of Recurrent Neural Networks (RNNs) designed to address the vanishing gradient problem inherent in standard RNNs, which hampers the ability to learn long-term dependencies \cite{Hochreiter1997}.

\subsubsection{Theoretical Background}
Unlike standard feedforward networks, RNNs maintain an internal state (memory) that processes sequences of inputs. However, during backpropagation through time, gradients in standard RNNs tend to decay exponentially, making it difficult to propagate error signals over long sequences.

The LSTM architecture introduces a memory cell $C_t$ modulated by three distinct gating mechanisms: the forget gate $f_t$, the input gate $i_t$, and the output gate $o_t$. These gates regulate the flow of information, allowing the network to retain relevant historical information while discarding noise.

The transition equations for a single LSTM cell at time step $t$ are defined as follows:

\begin{align}
    f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
    i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
    \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
    C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
    o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
    h_t &= o_t \odot \tanh(C_t)
\end{align}

where $\sigma$ denotes the sigmoid activation function, $\odot$ represents element-wise multiplication, $x_t$ is the input vector, and $h_t$ is the hidden state vector. This gating structure enables the gradient to flow unmodified through the cell state $C_t$, effectively mitigating the vanishing gradient problem.

\subsubsection{Network Architecture}
The implemented model architecture is a stacked LSTM network designed for regression. It consists of the following layers:
\begin{itemize}
    \item \textbf{Input Layer:} Accepts a sequence of shape $(k, 1)$, where $k$ is the lookback window size.
    \item \textbf{Hidden Layers:} Three stacked LSTM layers with 50 units each. The first two layers return full sequences to preserve temporal structure, while the final LSTM layer returns only the last hidden state vector.
    \item \textbf{Dropout:} A dropout rate (e.g., 0.2) is applied between layers to prevent overfitting.
    \item \textbf{Output Layer:} A single Dense (fully connected) layer with a linear activation function to produce the scalar forecast $\hat{y}_{t+1}$.
\end{itemize}
The model is optimized using the Adam algorithm, minimizing the Mean Squared Error (MSE) loss function.

\subsubsection{Preprocessing}
Financial time series are often non-stationary. To stabilize the mean and variance, raw OHLCV closing prices $P_t$ are transformed into logarithmic returns:
\begin{equation}
    r_t = \ln\left(\frac{P_t}{P_{t-1}}\right)
\end{equation}
Subsequently, the data is normalized to the range $[-1, 1]$ using Min-Max scaling to ensure stable convergence during gradient descent.

\subsubsection{Training and Inference}
The training process utilizes a sliding window technique. For a chosen lookback period $k$, the model learns to map a sequence of historical returns $X_t = \{r_{t-k}, \dots, r_{t-1}\}$ to the next target return $r_t$. Inference is performed in a walk-forward manner, ensuring that predictions at time $t$ rely solely on information available up to $t-1$.

\subsection{Experimental Design}
To validate the efficacy of hybrid strategies, we designed an experiment comparing three distinct strategy archetypes across two contrasting asset classes.

\subsubsection{Strategy Archetypes}
\begin{enumerate}
    \item \textbf{Statistical Control:} Strategies relying solely on deterministic logic (Asset $\to$ Indicator $\to$ Condition $\to$ Signal). Examples include SMA crossovers and RSI thresholds. This establishes a baseline for traditional algorithmic trading performance.
    \item \textbf{AI-Only Baseline:} A direct pipeline where the LSTM prediction triggers trades. A signal is generated if the predicted future price $\hat{p}_{t+n}$ exceeds the current price $p_t$ by a defined threshold. This measures the raw predictive power of the model without constraints.
    \item \textbf{Hybrid Experimental:} Strategies that gate AI predictions with statistical filters \cite{vanbekkum2021modular}. A trade is only executed if the AI predicts a move \textit{AND} a statistical condition confirms the market regime (e.g., Price $>$ SMA 200). This tests the hypothesis that technical analysis can reduce false positives in ML predictions.
\end{enumerate}

\subsubsection{Datasets}
We selected two datasets to test performance under different volatility regimes:
\begin{itemize}
    \item \textbf{Bitcoin (BTC/USD, 2020--2025):} Represents a high-volatility, noisy environment with frequent regime shifts.
    \item \textbf{Lockheed Martin (LMT, 2024--2025):} Represents a stable, low-volatility equity environment to test model generalization outside the cryptocurrency domain.
\end{itemize}

Approximately 340 unique configurations were backtested across these datasets to analyze the impact of different filter types and prediction horizons.
