\section{Preliminary Work}
For the purposes of this project, the focus was primarily aimed at analysis of financial market data. While multiple domains were considered for evaluating hybrid time series prediction strategies, financial market data was ultimately selected due to practical reasons. A large amount of high-quality historical price data is mostly freely available from various sources, and this accessibility and reduced data acquisition challenges made it a primary choice.

The aim of the project, however, is not necessarily limited to financial analysis. Various types of time series can be passed to constructed hybrid models, regardless of their domain of origin. While some models may specialize in certain domains, or be trained to predict more accurately on certain markets, the fundamental principle of hybrid strategies will likely carry over to other domains.

The decision to utilize LSTM networks over more contemporary architectures, such as Transformer-based models, was primarily driven by the balance between predictive performance and implementation complexity. Although newer models may have offered improvements in accuracy or more interesting results, they would probably have required significantly larger datasets, computational resources, and more specialized knowledge to implement and train properly. For the scope of this project, the primary research objective is the evaluation of the hybrid strategy framework itself, rather than the development of novel deep learning architectures. LSTMs showcase a well-established standard in time series forecasting with extensive documentation and library support, which would allow quick integration into the platform and ensure that development efforts remain focused on the system's modularity and the interaction between the machine learning and quantitative components.

To address the limitations of linear statistical models, the system employs a LSTM network. LSTMs are chosen specifically for their ability to retain information over long sequences, mitigating the "vanishing gradient" problem inherent in standard Recurrent Neural Networks \cite{Hochreiter1997}.\todo{Forklar ud fra diagram over LSTM}

The training process is structured as a supervised learning task. The raw OHLCV\todo{abbreviation consistency} data, specifically the closing price, which is converted to a logarithmic return and is restructured using a sliding window technique, for a chosen lookback period $k$, the model learns to map a sequence of historical prices $X_t = \{x_{t-k}, \dots, x_{t-1}\}$ to the next target value $x_t$. To ensure that the model converges properly during training, the input data is normalized to the range $[0, 1]$ using Min-Max scaling, preventing large price values from destabilizing the weights of the neural network. \todo{kilde indsætter jeg snart} \todo{Paragraf kan sammenslås med Method afsnittet}

The network architecture is constructed using the Keras functional API with TensorFlow as the backend. It consists of LSTM layers designed to extract temporal features, followed by fully connected Dense layers that map these features to a scalar output. The model is optimized using the Adam algorithm, minimizing the Mean. \todo{kilde indsætter jeg snart}

\todo{Sektion om finansielle termer relevant for forståelse af resultatforklaring}
% forklaring af MA, SMA, EMA, RSI, momentum, beta, drawdown, candlestick / bar, pnl / running balance